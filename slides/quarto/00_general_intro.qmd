---
title: "Deep Learning for System Identification"
subtitle: "Introduction"
author: "Marco Forgione"
institute: "IDSIA USI-SUPSI, Lugano, Switzerland"
aspectratio: 169
theme: moloch
pdf-engine: lualatex
slide-level: 2
themeoptions:
  - block=fill
mathfont: NewCMMath-Regular.otf
include-in-header:
  text: |
    \usepackage[most]{tcolorbox}
    \usepackage{xcolor}
    \usepackage{pdfpages}
    \DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
    \newcommand{\NN}{\mathbb{N}}
    \newcommand{\RR}{\mathbb{R}}
    \newcommand{\indep}{\perp \!\!\! \perp}
    \newcommand{\red}[1]{\textcolor{red}{#1}}
    \definecolor{dgreen}{RGB}{0,100,0}
    \definecolor{dyel}{RGB}{155,155,0}
format: 
  beamer:
    classoption: presentation
  beamer-handout:
    classoption: handout
echo: true
---


```{python}
#| echo: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

custom_params = {
    'figure.figsize': (4, 1.2),  # Width, Height in inches
    'font.size': 7,           # Default font size
    'axes.grid': True,         # Always show grid
}
plt.rcParams.update(custom_params)
```

## Artificial Intelligence, Machine Learning, Deep Learning
\begin{center}
  \includegraphics[width=0.99\textwidth]{img/ai_subsets.png}
\end{center}

## Machine learning: Regression
Nothing really special...

:::: {.columns}
::: {.column width="55%"}
\vskip 1em

* Dataset: $\qquad \qquad D = \{(x_i, y_i)\}_{i=1}^N$ \vskip 0.6em
* Model structure: $\;\qquad \hat y = M(x; \red{\theta})$ \vskip 0.6em
* Loss: $\;\;\mathcal{L}(\red{\theta}) = \frac{1}{N} \sum_{i=1}^N 
\|y_i - \hat y_i(\red{\theta})\|^2\qquad$ \vskip 0.6em


\begin{center}
\vskip -1em
$$
\hat \theta = \arg \min_{\red{\theta}} \mathcal{L}(\red{\theta})
$$
\end{center}
:::

::: {.column width="45%"}
```{python}
#| echo: false 
sizes = np.array([50.0, 75.5, 99.3, 149.8, 190.4, 200.8, 200.0, 300.0])
prices = np.array([83.3, 125.3, 189.2, 295.1, 644.0, 660.8, 693.6, 1189.5])

# Linear fit
coef = np.polyfit(sizes, prices, 2)

# Plot
plt.figure(figsize=(4, 2.5))
plt.title("Real estate application")
plt.scatter(sizes, prices, label="Data")

x_line = np.linspace(sizes.min(), sizes.max(), 100)
y_line = np.polyval(coef, x_line)
plt.plot(x_line, y_line, color="black", label="Model")

plt.xlabel("House size ($\\mathrm{m}^2$)")
plt.ylabel("Price [kEUR]")
plt.legend()
plt.tight_layout()
```
:::
::::

\pause
\vskip 1em


* Expressive model structures like neural networks \pause
* Iterative optimization, often gradient-based \pause
* Efficient software for automatic differentiation \pause

Still, you may see it as *glorified curve fitting*.

## Feed-Forward Neural Networks

:::: {.columns}
::: {.column width="55%"}
\begin{center}
  \includegraphics[width=0.99\textwidth]{img/feedforward.png}
\end{center}
:::

::: {.column width="45%"}
 \begin{align*}
\textcolor{dgreen}{z_1^2} &= \sigma\left(\sum_{j=1}^4 w^1_{1,j}\textcolor{red}{z^1_j} + b_1^1\right) \\
\textcolor{blue}{z_2^3} &= \sigma\left(\sum_{j=1}^5 w^2_{2,j}\textcolor{dgreen}{z^2_j} + b^2_2\right) \\
\textcolor{dyel}{y}&\textcolor{dyel}{=z^4} = \sum_{j=1}^3 w^3_{1,j}\textcolor{blue}{z^3_j} + b^3
 \end{align*}
:::
::::
\pause
\begin{align*}
{y} = W_3\sigma\big(W_2\sigma(W_1 x + b_1)+ b_2\big) + b_3 = \mathrm{FF}(x;\theta)
\end{align*}

\pause
* Linear blocks interleaved by element-wise non-linearities (activation functions) \pause
* Non-linearities essential for expressivenes

## Gradient Descent

:::: {.columns}
::: {.column width="65%"}
1. Initialize: $\hat \theta$

2. **for** $k=1,\dots,N$:
   - Compute Gradient: $g = \nabla \mathcal{L}(\theta^k)$
   - Update Parameters: 
   $$\hat \theta = \hat \theta - \gamma g$$
3. **end**

\pause
\vskip 1em
Local convergence to one of the many minima is OK!
:::

::: {.column width="35%"}
LOSS_LANDSCAPE
:::
::::

\pause
\vskip 1em

* Mini-batching: at each iteration, compute loss on **a subset** of the dataset
* Variants of plain gradient descent like Adam are more common and effective
* Second-order methods like (L)-BFGS also suitable for small/medium-scale problems

## Example: synthetic toy dataset

:::: {.columns}
::: {.column width="55%"}
Consider the 2D function $f: \mathbb{R}^2 \mapsto \mathbb{R}$

$$ f(x) = 2\sin (x_1) - 3\cos (x_2) $$
$$ x \in [-2, 2]^2 \subset \mathbb{R}^2$$

\vskip 1em

* Training and test datasets: 500 random points in the domain
* Additive noise with standard deviation 0.1
:::

::: {.column width="45%"}
```{python}
#| echo: false
n_x = 2 # number of inputs
n_y = 1 # number of outputs
a = -2.0 # lower bound x_1/x_2
b = 2.0 # upper bound x_1/x_2
n_samples = 500 # number of samples in the training/test datasets
sigma_e = 0.1 # standard deviation of the noise
grid_points = 100 # number of points in the grid for the plot


def f(x):
    return 2*np.sin(x[..., 0])  - 3*np.cos(x[..., 1]) # ellipses used to handle an optional "batch" dimension
# f(np.tensor([0.2, 0.4])), 2*np.sin(0.2) - 3*np.cos(0.4) # test


x1_train = a + np.random.rand(n_samples)*(b - a)
x2_train = a + np.random.rand(n_samples)*(b - a)
X_train = np.stack([x1_train, x2_train], axis=-1)

y_train = f(X_train) + sigma_e * np.random.randn(n_samples)
y_train = y_train.reshape(-1, 1)
X_train.shape, y_train.shape 

x1_test = a + np.random.rand(n_samples)*(b - a)
x2_test = a + np.random.rand(n_samples)*(b - a)
X_test = np.stack([x1_test, x2_test], axis=-1)

y_test = f(X_test) + sigma_e * np.random.randn(n_samples)
y_test = y_test.reshape(-1, 1)
X_test.shape, y_test.shape 

## visualization
x1_grid = np.linspace(a, b, grid_points)
x2_grid = np.linspace(a, b, grid_points)
X1_mesh, X2_mesh = np.meshgrid(x1_grid, x2_grid)
X_grid = np.c_[X1_mesh.ravel(), X2_mesh.ravel()]
y_grid = f(X_grid)

fig = plt.figure(figsize=(5, 5))
ax1 = fig.add_subplot(121, projection='3d')
ax1.plot_surface(X1_mesh, X2_mesh, y_grid.reshape(100, 100), cmap='coolwarm', alpha=0.7)#, edgecolor='none')
ax1.set_xlabel("$x_1$")
ax1.set_ylabel("$x_2$")
ax1.scatter(X_train[:, 0], X_train[:, 1], y_train[:, 0], color='k', s=10)
ax1.legend()
ax1.set_zlabel("$y$")
plt.show()
```
:::
::::